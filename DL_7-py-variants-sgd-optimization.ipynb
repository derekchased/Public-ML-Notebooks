{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of stochastic gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setting the scene\n",
    "2. Stochastic Gradient Descent (SGD)\n",
    "3. SGD with adaptive learning rate\n",
    "4. SGD with momentum \n",
    "5. SGD with accumulated squared gradient: AdaGrad\n",
    "6. SGD with accumulated squared gradient: RMSProp\n",
    "7. SGD with accumulated squared gradient: ADAM\n",
    "8. SGD with Newtonâ€™s method\n",
    "9. SGD with the conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import matplotlib.lines as lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During learning, we optimize the mean squared error $MSE$ of the models $m$ for the model parameters $\\mathbf w$: \n",
    "\n",
    "$$MSE(\\mathbf w,m,X,Y) =\\frac{1}{N}\\sum_{i=1}^{N}(y_i - m(\\mathbf w, \\mathbf  x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we find $\\arg\\min_{\\mathbf w} MSE(\\mathbf w, m, X, Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse some functions defined in the notebook \"Learn an XOR Neural Network using gradient-based optimization\" including:\n",
    "1. The mean squared error function `mse` (loss function).\n",
    "2. The gradient of the mean squared error function `grad_mse` (gradient of the loss function).\n",
    "3. The gradient descent function for the loss function `grad_desc_mse`.\n",
    "4. The 3D surcace and contour plot function `plot3D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(ws, m, X, Y):\n",
    "    N = X.shape[0]\n",
    "    err = 0;\n",
    "    for i in range(N):\n",
    "        xi=X[i,:]\n",
    "        yi=m(ws,xi)\n",
    "        err = err + (Y[i]-yi)**2;\n",
    "    err = err/N;\n",
    "    return err\n",
    "\n",
    "def grad_mse(ws, m, grads, X, Y):\n",
    "    N = X.shape[0]\n",
    "    M = ws.shape[0]\n",
    "    grad_ws = np.zeros(M)\n",
    "    \n",
    "    for i in range(N):\n",
    "        xi = X[i,:]\n",
    "        yi = Y[i]\n",
    "        tmp = yi - m(ws,xi)\n",
    "        for j in range(M):\n",
    "            grad_ws[j] = grad_ws[j] + tmp * grads[j](ws,xi)\n",
    "    grad_ws = -2/N*grad_ws;\n",
    "    return grad_ws\n",
    "\n",
    "def grad_desc_mse(K, ws, learning_eps, loss, grad_loss, verbose=False, ax=None):\n",
    "    history = [ loss(ws) ]\n",
    "    for k in range(K):\n",
    "        grad_ws = grad_loss(ws)\n",
    "        old_ws = ws\n",
    "        ws = old_ws - learning_eps * grad_ws;\n",
    "        history.append(loss(ws))\n",
    "        if verbose:\n",
    "            plt.plot([old_ws[0],ws[0]],[old_ws[1],ws[1]], c=\"b\", linewidth=1)\n",
    "    return ws, history\n",
    "\n",
    "def plot3d(f, A, B, real3d=True):\n",
    "    Z = arrayfun(f,A,B)\n",
    "    if real3d:\n",
    "        fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "        surf = ax.plot_surface(A, B, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "    else:\n",
    "        cs = plt.contour(A,B,Z)\n",
    "        plt.clabel(cs, inline=True, fontsize=9)\n",
    "#     return Z, fig, ax\n",
    "    \n",
    "def arrayfun(f,A,B):\n",
    "    Z = np.zeros(A.shape)\n",
    "    for row in range(A.shape[0]):\n",
    "        for col in range(A.shape[1]):\n",
    "            Z[row,col] = f(A[row,col],B[row,col])\n",
    "    return Z\n",
    "\n",
    "def arrayfun3(f,A,B,C):\n",
    "    Z = np.zeros(A.shape)\n",
    "    for i in range(A.shape[0]):\n",
    "        Z[i] = f(A[i],B[i],C[i])\n",
    "    return Z\n",
    "\n",
    "def fullfact(levels):\n",
    "    \"\"\"\n",
    "    @Author: https://github.com/tisimst/pyDOE/blob/436143702507a5c8ff87b361223eee8171d6a1d7/pyDOE/doe_factorial.py\n",
    "    \"\"\"\n",
    "    n = len(levels)  # number of factors\n",
    "    nb_lines = np.prod(levels)  # number of trial conditions\n",
    "    H = np.zeros((nb_lines, n))\n",
    "    \n",
    "    level_repeat = 1\n",
    "    range_repeat = np.prod(levels)\n",
    "    for i in range(n):\n",
    "        range_repeat //= levels[i]\n",
    "        lvl = []\n",
    "        for j in range(levels[i]):\n",
    "            lvl += [j]*level_repeat\n",
    "        rng = lvl*range_repeat\n",
    "        level_repeat *= levels[i]\n",
    "        H[:, i] = rng\n",
    "     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate sample data points. The generator function is $y=20x_1 -3x_2$. We sample data at integer points $x_{i,1}\\times x_{i,2} \\in [1\\ldots N]\\times [1\\ldots N], N=100$ and add a random error to $y_i$ that is normally distributed proportional to $\\mathcal{N}(0,10)$. \n",
    "\n",
    "Below we plot the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(3)\n",
    "\n",
    "N=100 # N=1000;\n",
    "\n",
    "XX = np.fliplr(fullfact([N,N]));\n",
    "a10 = 20\n",
    "a20 = -3 \n",
    "X1 = XX[:,0]\n",
    "X2 = XX[:,1]\n",
    "\n",
    "f0 = lambda x1,x2: a10*x1 + a20*x2\n",
    "f = lambda x1,x2,r: a10*x1 + a20*x2 + r\n",
    "R = rng.normal(0, 1, (1,N*N)).T\n",
    "Y = arrayfun3(f,X1,X2,R)\n",
    "interval = np.arange(0,N,N/10)\n",
    "A,B = np.meshgrid(interval, interval)\n",
    "ff = lambda x1,x2: f(x1,x2,rng.normal(0, 10))\n",
    "plot3d(ff,A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we use throughout this notebook is a simple linear Neural Network model. It consists of one neuron connected to the input $\\mathbf x$ and an identity, i.e., no effective, activation function. The neuron and the whole model $m_1$ implements $m_1(\\mathbf w, \\mathbf x) = \\mathbf w^T \\mathbf x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1 = lambda ws,x: ws[0]*x[0] + ws[1]*x[1]\n",
    "mse1 = lambda ws: mse(ws,m1,XX,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Tensorflow default, we implement the Glorot uniform initializer for setting the initial weights $\\mathbf w_0$.\n",
    "It draws samples from a uniform random distribution within $[-\\mathit{limit}, \\mathit{limit}]$,\n",
    "where $\\mathit{limit} = \\sqrt{\\frac{6}{\\mathit{in} + \\mathit{out}}}$, and\n",
    "where $\\mathit{in}$ and $\\mathit{out}$ is the number of input and output units, resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23791112 0.3523239 ]\n"
     ]
    }
   ],
   "source": [
    "_in = 2;\n",
    "out = 1;\n",
    "limit = np.sqrt(6 / (_in + out))\n",
    "ws0 = np.array([rng.uniform()*2*limit-limit, rng.uniform()*2*limit-limit])\n",
    "print(ws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assess the loss $MSE$ of $m_1$ for this initial weights setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994568.9221731804"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse1(ws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $MSE(\\mathbf w)$ for any $\\mathbf w$ is defiend as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{MSE}(\\mathbf w) &= \\left [ \n",
    "\\frac{\\partial MSE(\\mathbf w)}{\\partial w_1},\n",
    "\\frac{\\partial MSE(\\mathbf w)}{\\partial w_2}\n",
    "\\right ]^T\\\\\n",
    "&= \\frac{1}{N}\\left [ \n",
    "\\frac{\\partial \\sum_{i=1}^{N}(y_i - m_1(\\mathbf w,\\mathbf x_i)^2}{\\partial w_1}, \n",
    "\\frac{\\partial \\sum_{i=1}^{N}(y_i - m_1(\\mathbf w,\\mathbf x_i))^2}{\\partial w_2} \n",
    "\\right ]^T \\\\\n",
    "&= \\frac{1}{N} \\left [ \n",
    "\\sum_{i=1}^{N}2(y_i- m_1(\\mathbf w,\\mathbf x_i))\\frac{-\\partial m_1(\\mathbf w,\\mathbf x_i)}{\\partial w_1},\n",
    "\\sum_{i=1}^{N}2(y_i- m_1(\\mathbf w,\\mathbf x_i))\\frac{-\\partial m_1(\\mathbf w,\\mathbf x_i)}{\\partial w_2}\n",
    "\\right ]^T \\\\\n",
    "&= -\\frac{2}{N} \\left [ \n",
    "\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w,\\mathbf x_i))\\frac{\\partial m_1(\\mathbf w,\\mathbf x_i)}{\\partial w_1},\n",
    "\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w,\\mathbf x_i))\\frac{\\partial m_1(\\mathbf w,\\mathbf x_i)}{\\partial w_2}\n",
    "\\right ]^T \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plug in the function $m_1$ and the first derivative of $m_1$ wrt. $w_1$ and $w_2$ resp.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial m_1(\\mathbf w,\\mathbf x_i)}{\\partial w_1} &=\\frac{\\partial \\mathbf w^T\\mathbf x_{i}}{\\partial w_1} = \\frac{\\partial w_1 x_{i,1} + w_2 x_{i,2}}{\\partial w_1}  =x_{i,1}\\\\\n",
    "\\frac{\\partial m_1(\\mathbf w,\\mathbf x_i)}{\\partial w_2} &=\\frac{\\partial \\mathbf w^T\\mathbf x_{i}}{\\partial w_2} = \\frac{\\partial w_1 x_{i,1} + w_2 x_{i,2}}{\\partial w_2}  = x_{i,2}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already defined the function $m_1$. Let's also define the functions of the first derivative of $m_1$ wrt. $w_1$ and $w_2$ resp. For the sake of generality, we keep $\\mathbf w$ as a formal parameter `ws` even though it is actually not needed for  derivatives of this concrete model $m_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients1 = [lambda ws,x: x[0], lambda ws,x: x[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient $\\nabla_{MSE}(\\mathbf w)$ has been defined using $m_1$ and the first derivative of $m_1$ wrt. $w_1$ and $w_2$, resp., as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent function optimizes $\\mathbf w$ by iterating over:\n",
    "\n",
    "$$\\mathbf w_{k+1}= \\mathbf w_k - \\varepsilon \\nabla_{MSE}(\\mathbf w_k)$$\n",
    "\n",
    "staring with $\\mathbf w_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we apply the gradient descent function minimizing $MSE$ for the model $m_1$. We plot the $MSE$ for each step $k$ in the iteration as a function of the current parameter setting $\\mathbf w_k$. We also marked the starting point $\\mathbf w_0$ with a `+` and the (ideal) minimum $(20, -3)$ with a `*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bd65977412dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlearning_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00015\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_desc_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_eps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0minterval\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mws0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mws0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0minterval2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mws0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mws0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot search space\n",
    "f = lambda a, b: mse1([a,b]) # convert to np array?\n",
    "\n",
    "# hold on\n",
    "plt.plot(a10,a20,'*r')\n",
    "plt.plot(ws0[0],ws0[1],'+r')\n",
    "\n",
    "# # gradient descent\n",
    "grad_loss = lambda ws: grad_mse(ws, m1, gradients1, XX, Y)\n",
    "K = 20; \n",
    "learning_eps = 0.00015; \n",
    "\n",
    "ws, history = grad_desc_mse(K, ws0, learning_eps, mse1, grad_loss, True, ax);\n",
    "interval   = np.arange( np.minimum(a10,ws0[0])-10, np.maximum(a10,ws0[0])+10, 1 )\n",
    "interval2  = np.arange( np.minimum(a20,ws0[1])-10, np.maximum(a20,ws0[1])+10, 1 )\n",
    "A,B = np.meshgrid(interval, interval2)\n",
    "plot3d(f, A, B, False) #3D contour\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history plot shows the convergence of the algorithm. After ca. $k=15$ iterations, the loss $MSE$ is almost zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(history)),history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD works like the gradient descent but computes the gradient based on a mini batch of size $m \\ll N$. Therefore, our SGD implementation `stochastic_grad_desc_mse` generates in each iteration an array `randices`, i.e., random indices  making up the current mini batch sample of the whole data set. We resample this array in each iteration of the SGD algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a corresponding `grad_mse2` function that computes the gradient only based on the mini batch data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stochastic_grad_desc_mse.m\n",
    "function [ws, history] = stochastic_grad_desc_mse(K, ws, learning_eps, loss, grad_loss, N, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        old_ws = ws;\n",
    "        ws= old_ws - learning_eps * grad_ws;\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% stochastic gradient descent\n",
    "grad_loss2 = @(ws, randices)(grad_mse(ws, m1, gradients1, XX(randices,:),Y(randices)));\n",
    "tic;\n",
    "[ws, history] = stochastic_grad_desc_mse(K, ws0, learning_eps, mse1, grad_loss2, N*N, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with adaptive learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate $\\varepsilon$ is adapted in each iteration, e.g., exponentially by $\\varepsilon_{k+1} = (1-\\alpha)\\varepsilon_{k}$ with $\\alpha$ a new hyper-parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stochastic_adaptive_grad_desc_mse.m\n",
    "function [ws, history] = stochastic_adaptive_grad_desc_mse(K, ws, learning_eps, loss, grad_loss, N, alpha, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        old_ws = ws;\n",
    "        ws= old_ws - learning_eps * grad_ws;\n",
    "        learning_eps = (1-alpha) * learning_eps;\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% stochastic adaptive gradient descent\n",
    "alpha = 0.03;\n",
    "tic;\n",
    "[ws, history] = stochastic_adaptive_grad_desc_mse(K, ws0, learning_eps, mse1, grad_loss2, N*N, alpha, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with momentum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient can be understood as a velocity at which we move towards the optimum. In this analogy, momentum $p$ is introduced as the velocity $v$ (gradient) times mass $m$, a new hyper-parameter of the algorithm. The velocity is set to the exponentially decaying moving average of past gradients. The initial velocity $v_0$ is the initial gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS!** The velocity (earlier gradient vectors) and the current gradient vector point \"uphill\", i.e, into the inverse direction of the expected minimum. They should, hence, be **both** subtracted from the current parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stochastic_momentum_grad_desc_mse.m\n",
    "function [ws, history] = stochastic_momentum_grad_desc_mse(K, ws, learning_eps, loss, grad_loss, N, mass, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    v = 0;\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        v = (v+grad_ws)/2;\n",
    "        old_ws = ws;\n",
    "        ws= old_ws - mass * v - learning_eps * grad_ws;\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% stochastic momentum gradient descent\n",
    "mass = 1/2 * 0.00015;\n",
    "learning_eps = 1/2 * 0.00015;\n",
    "tic;\n",
    "[ws, history] = stochastic_momentum_grad_desc_mse(K, ws0, learning_eps, mse1, grad_loss2, N*N, mass, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with accumulated squared gradient: AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With AdaGrad, each parameter has its own learning rate.\n",
    "This learning rate is always decreasing, but faster for the parameters with the larger gradients. The derease is quite fast, so we have to adapt the constant component $\\varepsilon$ of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ada_grad_mse.m\n",
    "function [ws, history] = ada_grad_mse(K, ws, learning_eps, loss, grad_loss, N, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    r = zeros(length(ws),1);\n",
    "    delta = 1e-10*ones(length(ws),1);\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        old_ws = ws;\n",
    "        r = r + grad_ws .* grad_ws; \n",
    "        ws= old_ws - learning_eps/(delta+sqrt(r)) .* grad_ws;\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% AdaGrad\n",
    "learning_eps = 15;\n",
    "tic;\n",
    "[ws, history] = ada_grad_mse(K, ws0, learning_eps, mse1, grad_loss2, N*N, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with accumulated squared gradient: RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp adds a hyper-parameter $\\rho \\in [0\\ldots 1]$ to AdaGrad. It biases between the accumulated squared gradients $r$, i.e., the historic (squared) velocity, and the current squared gradient before deviding the learning rate by $\\sqrt{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file rms_prob_mse.m\n",
    "function [ws, history] = rms_prob_mse(K, ws, learning_eps, loss, grad_loss, N, rho, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    r = zeros(length(ws),1);\n",
    "    delta = 1e-10*ones(length(ws),1);\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        old_ws = ws;\n",
    "        r = rho*r + (1-rho) * grad_ws .* grad_ws; \n",
    "        ws= old_ws - learning_eps/(delta+sqrt(r)) .* grad_ws;\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RMSProb, it was hard to find good hyper-parameters just by trying out a few. Here the loss for different hyper-parameter settings of $\\rho \\in [0\\ldots 1]$ and $\\varepsilon \\in [1\\ldots 20]$. We finally chose the setting with the minimum loss after 20 iterations: $\\varepsilon = 1, \\rho=0.98$. Note, that we needed to increase the number of iterations to arrive at a the ideal parameters. So, for this simplistic setup, RMSProb did not speed up convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/rms_prob_hyperparameters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% RMSProb; \n",
    "learning_eps = 1;\n",
    "rho = 0.98;\n",
    "K = 40; %!\n",
    "tic;\n",
    "[ws, history] = rms_prob_mse(K, ws0, learning_eps, mse1, grad_loss2, N*N, rho, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:20,history(1:20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with accumulated squared gradient: ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM seems to be the current choice of many. However, it hasn't been trivial to choose the learning rate and the two hyper-parameters $\\rho_1, \\rho_2$. We even needed to increase the number of iterations to arrive at a the ideal parameters. So, for this simplistic setup, ADAM did not speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file adam_mse.m\n",
    "function [ws, history] = adam_mse(K, ws, learning_eps, loss, grad_loss, N, rho1, rho2, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    s = zeros(length(ws),1);\n",
    "    r = zeros(length(ws),1);\n",
    "    t = 0;\n",
    "    delta = 1e-10*ones(length(ws),1);\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        old_ws = ws;\n",
    "        t = t +1;\n",
    "        s = rho1*s + (1-rho1) * grad_ws; \n",
    "        r = rho2*r + (1-rho2) * grad_ws .* grad_ws; \n",
    "        s_hat = s/(1-rho1^t);\n",
    "        r_hat = r/(1-rho2^t);\n",
    "        ws= old_ws - (learning_eps*s_hat)/(delta+sqrt(r));\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% ADAM; \n",
    "learning_eps = 0.1; \n",
    "rho1 = 0.9;\n",
    "rho2 = 0.999;\n",
    "K=400; %!!\n",
    "tic;\n",
    "[ws, history] = adam_mse(K, ws0, learning_eps, mse1, grad_loss2, N*N, rho1, rho2, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:20,history(1:20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Newtonâ€™s method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obviously difficult to set the learning rate (and the other parameters) right. To help this, Newton's method (and other second order methods) involve the second order derivatives, e.g., the Hessian matrix $H_{MSE}$. Then they iterate:\n",
    "\n",
    "$$\n",
    "\\mathbf w_{k+1} = \\mathbf w_k - H_{MSE}(\\mathbf w_k)^{-1} \\nabla_{MSE}(\\mathbf w_k) \n",
    "$$\n",
    "\n",
    "where $H_{MSE}$ is the Hessian matrix of $MSE$. \n",
    "\n",
    "$H_{MSE}$ is defined as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{MSE}(\\mathbf w) &= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 MSE(\\mathbf w)}{\\partial w_1^2} & \\frac{\\partial^2 MSE(\\mathbf w)}{\\partial w_1 \\partial w_2} \\\\\n",
    "\\frac{\\partial^2 MSE(\\mathbf w)}{\\partial w_2 \\partial w_1} & \\frac{\\partial^2 MSE(\\mathbf w)}{\\partial w_2^2} \n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\frac{-2}{N}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial (\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w, \\mathbf x_i)x_{i,1})}{\\partial w_1} & \n",
    "\\frac{\\partial (\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w, \\mathbf x_i)x_{i,1})}{\\partial w_2} \\\\\n",
    "\\frac{\\partial (\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w, \\mathbf x_i)x_{i,2})}{\\partial w_1} & \n",
    "\\frac{\\partial (\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w, \\mathbf x_i)x_{i,2})}{\\partial w_2} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\frac{2}{N}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^{N}x_{i,1}^2 & \\sum_{i=1}^{N}x_{i,1}x_{i,2} \\\\\n",
    "\\sum_{i=1}^{N}x_{i,1}x_{i,2} & \\sum_{i=1}^{N} x_{i,2}^2\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This approach generalizes easily to a stochastic version of Newtons method using only a sample of the training data of size $m \\ll N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file hessian_mse.m\n",
    "function H = hessian_mse(ws, X)\n",
    "    N = length(X);\n",
    "    M = length(ws);\n",
    "    H = zeros(M);\n",
    "    for r=1:M\n",
    "        for c=1:M\n",
    "            for i=1:N\n",
    "                H(r,c) = H(r,c) + X(i,r)*X(i,c);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    H = 2/N*H;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad2_loss = @(ws, randices)(hessian_mse(ws, XX(randices,:)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stochastic_newton_mse.m\n",
    "function [ws, history] = stochastic_newton_mse(K, ws, loss, grad_loss, grad2_loss, N, verbose)\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        old_ws = ws;\n",
    "        H = grad2_loss(ws, randices);\n",
    "        ws= old_ws - inv(H) * grad_ws;\n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% Newton's method; \n",
    "K=1; %!!!\n",
    "tic;\n",
    "[ws, history] = stochastic_newton_mse(K, ws0, mse1, grad_loss2, grad2_loss, N*N, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with the conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian is quite large, with a size $O(n^2)$ for $n$ parameters, and needs to be updated in each iteration. The conjugate gradient method tries to avoid the explicite calculation of the Hessian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the algorithm is a search along the (adjusted) gradient line for the optimum (lowest loss) point on this line. Therefore, we would like to use the mini batch instead of the whole training data set. We define a stochastic loss function accordingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse2 = @(ws, randices)(mse(ws,m1,XX(randices,:),Y(randices)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *linear* conjugate gradient descent, we search along the gradient line for the point to adjust the parameters to. For *non-linear* conjugate gradient descent, we adjust the gradient line before this linear search based on the gradients' history. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS!** For non-linear conjugate gradient descent, the gradient line adjustment (actually computing its scaling factor $\\beta$) in iteration $k$ requires the gradient of iteration $k-1$. This gradient is not available in the first iteration. Therefore, we need to do a linear conjugate gradient descent in the first iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file cg_mse.m\n",
    "function [ws, history] = cg_mse(K, ws, loss, stochastic_loss, grad_loss, N, verbose)\n",
    "    %initialization\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    %we only need variables at t (var) and t-1 (var_old) \n",
    "    rho_old = zeros(length(ws),1);\n",
    "    grad_ws_old = zeros(length(ws),1);\n",
    "    \n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        %Compute gradient\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        if k==1\n",
    "            %Linear cg, i.e., no search direction adjustment in the first round\n",
    "            rho = - grad_ws;\n",
    "        else\n",
    "            %Compute Polak-RibiÃ¨re\n",
    "            beta = ((grad_ws - grad_ws_old)' * grad_ws) /(grad_ws_old' * grad_ws_old);\n",
    "            %Compute search direction\n",
    "            rho = - grad_ws + beta*rho_old;\n",
    "        end\n",
    "        %Naive line search for epsilon* = argmin stochastic_loss(ws+epsilon * rho)\n",
    "        epsilon_star = 0.00001;\n",
    "        minimum_star = stochastic_loss(ws+epsilon_star*rho,randices);\n",
    "        for epsilon = 0.00001:0.0001:1\n",
    "            minimum = stochastic_loss(ws+epsilon*rho,randices);\n",
    "            if minimum < minimum_star\n",
    "                epsilon_star = epsilon;\n",
    "                minimum_star = minimum;\n",
    "            end\n",
    "        end\n",
    "        %Remember parameters (for drawing the line of this step if verbose) \n",
    "        old_ws = ws;\n",
    "        %Update the parameters\n",
    "        ws= old_ws + epsilon_star * rho;\n",
    "        %Remember the variables at t as the old variables at t-1 in the next iteration \n",
    "        rho_old = rho;\n",
    "        grad_ws_old = grad_ws;\n",
    "        %Draw the line \n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%%CG; \n",
    "K=20;\n",
    "tic;\n",
    "[ws, history] = cg_mse(K, ws0, mse1, mse2, grad_loss2, N*N, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line search that we implemented naÃ¯vely in the above algorithm is actually an optimization problem. Since, we only have one variable $\\varepsilon$ to optimize for, we could use the quickly converging Newton method to implement this optimization. \n",
    "\n",
    "Substituting $\\mathbf w \\rightarrow \\mathbf w + \\varepsilon\\boldsymbol{\\rho}$, we seek to find $\\varepsilon^* = \\arg\\min_{\\varepsilon} MSE(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},m,X,Y)$. This leads to the following function to optimize:\n",
    "\n",
    "$$MSE^*(\\varepsilon,\\mathbf w,\\boldsymbol{\\rho},m,X,Y) =\\frac{1}{N}\\sum_{i=1}^{N}(y_i - m(\\mathbf w+\\varepsilon\\boldsymbol{\\rho}, \\mathbf  x_i))^2$$\n",
    "\n",
    "Recall, that only $\\varepsilon$ is a variable in this function; all others are constant.\n",
    "\n",
    "To apply Newton's method, we find the first and second derivations of $MSE^*$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $MSE^*(\\varepsilon)$ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{MSE^*}(\\varepsilon) &= \\frac{\\partial MSE^*(\\varepsilon)}{\\partial \\varepsilon}\\\\\n",
    "&= \\frac{1}{N}\n",
    "\\frac{\\partial \\sum_{i=1}^{N}(y_i - m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i))^2}{\\partial \\varepsilon}\\\\\n",
    "&= \\frac{1}{N}  \n",
    "\\sum_{i=1}^{N}2(y_i- m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i))\\frac{-\\partial m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i)}{\\partial \\varepsilon}\\\\\n",
    "&= -\\frac{2}{N} \n",
    "\\sum_{i=1}^{N}(y_i- m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i))\\frac{\\partial m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i)}{\\partial \\varepsilon}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plug in the function $m_1$ and the first derivation of $m_1$ to $\\varepsilon$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i)}{\\partial \\varepsilon} &=\\frac{\\partial (\\mathbf w + \\varepsilon\\boldsymbol{\\rho})^T\\mathbf x_{i}}{\\partial \\varepsilon} = \\frac{\\partial (\\mathbf w^T \\mathbf x_{i} + \\varepsilon\\boldsymbol{\\rho}^T\\mathbf x_{i})}{\\partial \\varepsilon}  =\\boldsymbol{\\rho}^T\\mathbf x_{i}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second derivation of $MSE^*(\\varepsilon)$ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{MSE^*}(\\varepsilon) &= \\frac{\\partial^2 MSE^*(\\varepsilon)}{\\partial \\varepsilon^2}\\\\\n",
    "&= -\\frac{2}{N} \n",
    "\\frac{\\partial \\sum_{i=1}^{N}(y_i- m_1(\\mathbf w + \\varepsilon\\boldsymbol{\\rho},\\mathbf x_i))\\boldsymbol{\\rho}^T\\mathbf x_{i}}{\\partial \\varepsilon}\\\\\n",
    "&= -\\frac{2}{N} \n",
    "\\frac{\\partial \\sum_{i=1}^{N}(y_i- (\\mathbf w^T \\mathbf x_i + \\varepsilon\\boldsymbol{\\rho}^T\\mathbf x_i))\\boldsymbol{\\rho}^T\\mathbf x_{i}}{\\partial \\varepsilon}\\\\\n",
    "&= \\frac{2}{N} \n",
    "\\sum_{i=1}^{N}(\\boldsymbol{\\rho}^T\\mathbf x_{i})^2\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file grad_eps.m\n",
    "function grad_eps = grad_eps(eps, ws, rho, m, X, Y)\n",
    "    N = length(X);\n",
    "    grad_eps = 0;\n",
    "    for i=1:N\n",
    "        xi = X(i,:);\n",
    "        yi = Y(i);\n",
    "        tmp = yi - m(ws+eps*rho, xi);\n",
    "        grad_eps = grad_eps + tmp*(rho'*xi');\n",
    "    end\n",
    "    grad_eps = -2/N*grad_eps;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_grad_eps = @(eps, ws, rho, randices)(grad_eps(eps, ws, rho, m1, XX(randices,:), Y(randices)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file grad2_eps.m\n",
    "function grad2_eps = grad2_eps(rho, X)\n",
    "    N = length(X);\n",
    "    grad2_eps = 0;\n",
    "    for i=1:N\n",
    "        xi = X(i,:);\n",
    "        tmp = rho'*xi';\n",
    "        grad2_eps = grad2_eps + tmp^2;\n",
    "    end\n",
    "    grad2_eps = 2/N*grad2_eps;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_grad2_eps = @(rho, randices)(grad2_eps(rho, XX(randices,:)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file cg_mse_newton.m\n",
    "function [ws, history] = cg_mse_newton(K, ws, loss, grad_loss, stochastic_grad_eps, stochastic_grad2_eps, N, verbose)\n",
    "    %initialization\n",
    "    batch_size = N*0.01;\n",
    "    history(1) = loss(ws);\n",
    "    %we only need variables at t (var) and t-1 (var_old) \n",
    "    rho_old = zeros(length(ws),1);\n",
    "    grad_ws_old = zeros(length(ws),1);\n",
    "    \n",
    "    for k = 1:K\n",
    "        randices = randsample(1:N,batch_size,false);\n",
    "        %Compute gradient\n",
    "        grad_ws = grad_loss(ws, randices);\n",
    "        if k==1\n",
    "            %Linear cg, i.e., no search direction adjustment in the first round\n",
    "            rho = - grad_ws;\n",
    "        else\n",
    "            %Compute Polak-RibiÃ¨re\n",
    "            beta = ((grad_ws - grad_ws_old)' * grad_ws) /(grad_ws_old' * grad_ws_old);\n",
    "            %Compute search direction\n",
    "            rho = - grad_ws + beta*rho_old;\n",
    "        end\n",
    "        %Newton line search for epsilon* = argmin stochastic_loss(ws+epsilon * rho)\n",
    "        %Just one iteration as the (stochastic) loss function is quadratic in epsilon\n",
    "        epsilon_star = 0.5;\n",
    "        grad_eps = stochastic_grad_eps(epsilon_star, ws, rho, randices);\n",
    "        h = stochastic_grad2_eps(rho, randices);\n",
    "        epsilon_star = epsilon_star - grad_eps/h;\n",
    "\n",
    "        %Remember parameters (for drawing the line of this step if verbose) \n",
    "        old_ws = ws;\n",
    "        %Update the parameters\n",
    "        ws= old_ws + epsilon_star * rho;\n",
    "        %Remember the variables at t as the old variables at t-1 in the next iteration \n",
    "        rho_old = rho;\n",
    "        grad_ws_old = grad_ws;\n",
    "        %Draw the line \n",
    "        if verbose\n",
    "            line([old_ws(1),ws(1)],[old_ws(2),ws(2)]);\n",
    "        end\n",
    "        history(k+1) = loss(ws);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% plot search space\n",
    "plot3d(f, A, B, false) %3D contour\n",
    "hold on\n",
    "plot(a10,a20,'*r')\n",
    "plot(ws0(1),ws0(2),'+r')\n",
    "\n",
    "%% CG w Newton's method\n",
    "K=20;\n",
    "tic;\n",
    "[ws, history] = cg_mse_newton(K, ws0, mse1, grad_loss2, stochastic_grad_eps, stochastic_grad2_eps, N*N, true);\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(history),history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "565px",
    "left": "1097px",
    "right": "20px",
    "top": "120px",
    "width": "331px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
